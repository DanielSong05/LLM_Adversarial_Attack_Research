# LLM Adversarial Attack Research

Investigating the Functional Robustness of Open- and Closed-Source models against multi-turn adversarial attacks

---

## References

- [MHJ: LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet](https://arxiv.org/abs/2408.15221)
- [CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference](https://aclanthology.org/2024.emnlp-main.968)
- [BAD: Recipes for Safety in Open-domain Chatbots](https://arxiv.org/abs/2010.07079)
