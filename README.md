# LLM Adversarial Attack Research

Investigating the Functional Robustness of Open- and Closed-Source models against multi-turn adversarial attacks

---

## Overview
- Models: gpt-4-turbo, llama-3.3-70b-versatile, llama-3.1-8b-instant, claude-3-7-sonnet-20250219
- Benchmarks: MHJ, CoSafe
- Attack types: MHJ, Coreference
- Evaluation Critera: Attack Success Rate (ASR %), Adversarial cost (turn & token)

## References

- [MHJ: LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet](https://arxiv.org/abs/2408.15221)
- [CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference](https://aclanthology.org/2024.emnlp-main.968)
