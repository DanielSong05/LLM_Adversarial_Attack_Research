# LLM Adversarial Attack Research

Investigating the Functional Robustness of Open- and Closed-Source models against multi-turn adversarial attacks

---

## Overview
- Models: gpt-4-turbo, llama-3.3-70b-versatile, llama-3.1-8b-instant, claude-3-7-sonnet-20250219
- Benchmarks: MHJ, CoSafe, BAD datasets
- Attack types: Human-crafted, coreference, escalation
- Evaluation Critera: Attack Success Rate (ASR %), Severity (Toxicity Score), Adversarial cost (turn & token)

## References

- [MHJ: LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet](https://arxiv.org/abs/2408.15221)
- [CoSafe: Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference](https://aclanthology.org/2024.emnlp-main.968)
- [BAD: Recipes for Safety in Open-domain Chatbots](https://arxiv.org/abs/2010.07079)
